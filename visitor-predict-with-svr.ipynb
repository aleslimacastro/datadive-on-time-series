{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# read and normalize the messy columns from weather data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import datetime \n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# adjust all plot sizes to max size for juypter notebook\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# read data\n",
    "stringname = \"train\"\n",
    "weather1 = pd.read_csv('../data/raw/weather_dwd_' + stringname + '_set.csv')\n",
    "weather2 = pd.read_csv('../data/raw/weather_uni_osnabrueck_' + stringname + '_set.csv')\n",
    "weather2test = pd.read_csv('../data/raw/weather_uni_osnabrueck_test_set.csv')\n",
    "\n",
    "# weather information is messy, clean it up \n",
    "# use weather1 for days 0-2158 and weather2 from 537 days after start\n",
    "w2start = 537\n",
    "weatherX1 = weather1.ix[:2158,0:2].join(weather1.ix[:2158,3])\n",
    "weatherX2 = weather2.ix[:,0:7]\n",
    "\n",
    "# this is a cool command: apply the function (\"lambda\") to all values of column \"date\" \n",
    "weatherX2['date'] = weatherX2['date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime('%-m/%-d/%Y') )\n",
    "weatherX2 = weatherX2.ix[w2start:,0:2].join(weatherX2.ix[w2start:,4])\n",
    "\n",
    "weather2test = weather2test.ix[:,0:2].join(weather2test.ix[:,4])\n",
    "\n",
    "weatherX1.columns = ['date', 'humidity', 'temperature']\n",
    "weatherX2.columns = ['date', 'humidity', 'temperature']\n",
    "weather2test.columns = ['date', 'humidity', 'temperature']\n",
    "superWeather = pd.concat([weatherX1, weatherX2, weather2test])\n",
    "\n",
    "# save consolidated weather to csv\n",
    "#superWeather.to_csv(\"weather.csv\", sep=',', index=False)\n",
    "\n",
    "# load consolidated weather from csv\n",
    "#superWeather.from_csv(\"weather.csv\", sep=',')\n",
    "\n",
    "superWeather.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weatherX1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load visitor data from nettebad and the good columns from weather data, and the cleaned up messy columns from weather data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "def combine_input_files(stringname):\n",
    "    train_orig = pd.read_csv('../data/raw/nettebad_' + stringname + '_set.csv')\n",
    "\n",
    "    weather1 = pd.read_csv('../data/raw/weather_dwd_' + stringname + '_set.csv')\n",
    "\n",
    "    weather2 = pd.read_csv('../data/raw/weather_uni_osnabrueck_' + stringname + '_set.csv')\n",
    "\n",
    "    weatherX1 = weather1.ix[:,5:7]\n",
    "    weatherX2 = weather1.ix[:,0]\n",
    "    weatherX = weatherX1.join(weatherX2)\n",
    "    \n",
    "    train = pd.merge(train_orig, weatherX, on='date')\n",
    "    return train \n",
    "\n",
    "traindata = combine_input_files(\"train\")\n",
    "traindata = pd.merge(traindata, superWeather, on=\"date\")\n",
    "testdata = combine_input_files(\"test\")\n",
    "testdata = pd.merge(testdata, superWeather, on=\"date\")\n",
    "\n",
    "testdata.to_csv(\"testdata.csv\", sep=',', index=False)\n",
    "\n",
    "traindata.to_csv(\"traindata.csv\", sep=',', index=False)\n",
    "traindata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform to feature vectors\n",
    "\n",
    "import datetime \n",
    "\n",
    "def get_feature_vector(train):\n",
    "    X1 = train.ix[:,2:11]\n",
    "    X2 = train.ix[:,13:]\n",
    "    y = train.ix[:,1]\n",
    "    datums = train.ix[:,0]\n",
    "    wochentag = []\n",
    "    monat = []\n",
    "\n",
    "    for idx in range(len(X1)):\n",
    "        datum = datetime.datetime.strptime(datums[idx], \"%m/%d/%Y\") \n",
    "        #print datums[idx], datum.strftime(\"%Y-%m-%d\")\n",
    "        wochentag.append(datum.isoweekday())\n",
    "        monat.append(int(datum.strftime(\"%m\")))\n",
    "\n",
    "    dat1 = pd.DataFrame({'Monat': monat})\n",
    "    dat2 = pd.DataFrame({'Wochentag': wochentag})\n",
    "    Xre = X1.join(X2).join(dat1).join(dat2)\n",
    "    return [Xre,y]\n",
    "\n",
    "[X, y] = get_feature_vector(traindata)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same for test set\n",
    "\n",
    "def get_test_vector(train):\n",
    "    X1 = train.ix[:,1:10]\n",
    "    X2 = train.ix[:,12:]\n",
    "    datums = train.ix[:,0]\n",
    "    wochentag = []\n",
    "    monat = []\n",
    "\n",
    "    for idx in range(len(X1)):\n",
    "        datum = datetime.datetime.strptime(datums[idx], \"%m/%d/%Y\") \n",
    "        #print datums[idx], datum.strftime(\"%Y-%m-%d\")\n",
    "        wochentag.append(datum.isoweekday())\n",
    "        monat.append(int(datum.strftime(\"%m\")))\n",
    "\n",
    "    dat1 = pd.DataFrame({'Monat': monat})\n",
    "    dat2 = pd.DataFrame({'Wochentag': wochentag})\n",
    "    Xre = X1.join(X2).join(dat1).join(dat2)\n",
    "    return [Xre, datums]\n",
    "\n",
    "[X_test, dates_predict_set] = get_test_vector(testdata)\n",
    "print(len(X_test))\n",
    "X_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print dataset\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "X.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce the set size, remove nan\n",
    "from sklearn import preprocessing\n",
    "\n",
    "startAt = 0\n",
    "\n",
    "Xneu = X.ix[startAt:]\n",
    "yneu = y.ix[startAt:]\n",
    "# fix nan values\n",
    "Xneu = Xneu.interpolate()\n",
    "\n",
    "# we dont normalize. gives worse results than unnormalized data\n",
    "#Xneu = preprocessing.normalize(Xneu)\n",
    "Xneu.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING NOW (v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split test set\n",
    "\n",
    "# Python 3 syntax in Python 2\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Scikit learn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "train, test, target_train, target_test = \\\n",
    "    cross_validation.train_test_split(\n",
    "        Xneu,\n",
    "        yneu.as_matrix(), \n",
    "        test_size=0.2)\n",
    "    \n",
    "print( len(train) )\n",
    "print( len(test) )\n",
    "\n",
    "print( len(target_train) )\n",
    "print( len(target_test) )\n",
    "\n",
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRaINING V2 (poly\n",
    "# poly kernel is super slow, and doesnt give better results than rbf. dont use it\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# result: 343\n",
    "#  startAt 2500\n",
    "#  Parameters: {'kernel': 'poly', 'C': 100.0, 'degree': 2}\n",
    "\n",
    "\n",
    "# SVM needs some initial configuration parameters \n",
    "# (\"hyperparameters\") \n",
    "\n",
    "# Good hyperparameters can be obtained \n",
    "# by using grid search & \"cross validation\"\n",
    "\n",
    "# But we need to define the parameter search space\n",
    "\n",
    "parameters = {'kernel': ['poly'], \n",
    "              \"C\": [1e1, 1e2, 1e3],\n",
    "              'degree': [2],\n",
    "             # \"gamma\": np.logspace(-2, 2, 5)\n",
    "             }\n",
    "\n",
    "#   rbf = radial basis function = gauss kernel\n",
    "#   Other kernels are rarely needed, said Andrew NG \n",
    "#   (Associate Professor of Computer Science at Stanford, founder Coursera)\n",
    "\n",
    "\n",
    "# Search for the best classifier within the search space \n",
    "# A search consists of:\n",
    "#   an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "#   a parameter space;\n",
    "#   ...\n",
    "# Documentation: \n",
    "#   http://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "clf = grid_search.GridSearchCV(svm.SVR(), parameters)\n",
    "clf.fit(train, target_train)\n",
    "classifier = clf.best_estimator_\n",
    "\n",
    "print(\"Set sizes:\")\n",
    "print( len(train) )\n",
    "print( len(test) )\n",
    "\n",
    "print()\n",
    "print (metrics.mean_squared_error(target_test,classifier.predict(test)))\n",
    "print('Parameters:', clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training V4 - KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(Xneu, yneu)\n",
    "\n",
    "print (metrics.mean_squared_error(target_test,classifier.predict(test)))\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear regression with gradient boosting. best results!\n",
    "\n",
    "print(\"Set sizes:\")\n",
    "print( len(train) )\n",
    "print( len(test) )\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "# classifier = linear_model.LinearRegression()\n",
    "#classifier = linear_model.Ridge(alpha=1.0)\n",
    "\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.04, 'loss': 'ls'}\n",
    "\n",
    "\n",
    "parameters = {'max_depth': [ 4,5,6 ]}\n",
    "\n",
    "\n",
    "#classifier = ensemble.GradientBoostingRegressor(**params)\n",
    "#classifier.fit(Xneu, yneu)\n",
    "\n",
    "clf = grid_search.GridSearchCV(ensemble.GradientBoostingRegressor(**params), parameters)\n",
    "clf.fit(train, target_train)\n",
    "classifier = clf.best_estimator_\n",
    "\n",
    "\n",
    "print('Parameters:', clf.best_params_)\n",
    "\n",
    "MSE = metrics.mean_squared_error(target_test,classifier.predict(test))\n",
    "print (MSE**(0.5))\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "classifier = Lasso()\n",
    "classifier.fit(Xneu, yneu)\n",
    "\n",
    "MSE = metrics.mean_squared_error(target_test,classifier.predict(test))\n",
    "print (MSE**(0.5))\n",
    "\n",
    "classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRaINING V3: SVR with rbf kernel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Set sizes:\")\n",
    "print( len(train) )\n",
    "print( len(test) )\n",
    "\n",
    "# SVM needs some initial configuration parameters \n",
    "# (\"hyperparameters\") \n",
    "\n",
    "# Good hyperparameters can be obtained \n",
    "# by using grid search & \"cross validation\"\n",
    "\n",
    "# But we need to define the parameter search space\n",
    "\n",
    "parameters = {'kernel': ['rbf'], \n",
    "              'C': [  100, 1000, 10000 ],\n",
    "              'gamma': [ 0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "#   rbf = radial basis function = gauss kernel\n",
    "#   Other kernels are rarely needed, said Andrew NG \n",
    "#   (Associate Professor of Computer Science at Stanford, founder Coursera)\n",
    "\n",
    "\n",
    "# Search for the best classifier within the search space \n",
    "# A search consists of:\n",
    "#   an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "#   a parameter space;\n",
    "#   ...\n",
    "# Documentation: \n",
    "#   http://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "clf = grid_search.GridSearchCV(svm.SVR(), parameters)\n",
    "clf.fit(train, target_train)\n",
    "classifier = clf.best_estimator_\n",
    "\n",
    "\n",
    "print()\n",
    "print (metrics.mean_squared_error(target_test,classifier.predict(test)))\n",
    "\n",
    "print('Parameters:', clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print('Parameters:', clf.best_params_)\n",
    "print()\n",
    "print('Best classifier score')\n",
    "print(metrics.classification_report(\n",
    "        target_test,\n",
    "        classifier.predict(test).astype(int)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "predict_result = classifier.predict(X_test).astype(int)\n",
    "print (metrics.mean_squared_error(target_test,classifier.predict(test)))\n",
    "\n",
    "# draw regressor\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# print predict_result \n",
    "ts = pd.Series(predict_result)\n",
    "ts.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save result csv file for upload to kaggle\n",
    "\n",
    "# [dates_predict_set, predict_result]\n",
    "\n",
    "filename = \"resultGlinear6b.csv\"\n",
    "final_set = []\n",
    "target = open(filename, 'w')\n",
    "target.truncate()\n",
    "target.write('date,visitors_pool_total')\n",
    "target.write(\"\\n\")\n",
    "\n",
    "for idx in range(len(dates_predict_set)):\n",
    "    final_set.append( [dates_predict_set[idx], predict_result[idx]] )\n",
    "    target.write( dates_predict_set[idx] + \",\" + str( predict_result[idx]) )\n",
    "    target.write(\"\\n\")\n",
    "\n",
    "target.close()\n",
    "\n",
    "final_set        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "lw = 2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.hold('on')\n",
    "plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')\n",
    "#plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
